{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `a.` classification model to predict product category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x232122d2c90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.memory', '30g')\n",
    "conf.set('spark.driver.memory', '30g')\n",
    "# conf.set(\"spark.executor.instances\", 4)\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.master(\"local\").appName('Classification').config(conf=conf).getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- review_title: string (nullable = true)\n",
      " |-- reviewer_id: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      "\n",
      "+-------+--------+----------------+------------------+--------------------+----------+------------------+-------------------+------------------+\n",
      "|summary|language|product_category|        product_id|         review_body| review_id|      review_title|        reviewer_id|             stars|\n",
      "+-------+--------+----------------+------------------+--------------------+----------+------------------+-------------------+------------------+\n",
      "|  count|  200000|          200000|            200000|              200000|    200000|            200000|             200000|            200000|\n",
      "|   mean|    null|            null|              null|                null|      null|13.136363636363637|               null|               3.0|\n",
      "| stddev|    null|            null|              null|                null|      null| 36.48430546761916|               null|1.4142170979202575|\n",
      "|    min|      en|         apparel|product_en_0000010|! I was hoping th...|en_0000003|                 !|reviewer_en_0000001|                 1|\n",
      "|    max|      en|        wireless|product_en_0999988|ðŸ˜®..... SHOCK ðŸ˜«....|en_0999991|                ðŸ¥º|reviewer_en_0999996|                 5|\n",
      "+-------+--------+----------------+------------------+--------------------+----------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_location = './data/train_test/dataset_en_train.json'\n",
    "df = spark.read.json(file_location)\n",
    "df = df.na.drop()\n",
    "df.printSchema()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing idfferent lr approach\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StringIndexer, VectorAssembler, StopWordsRemover\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "training_data, test_data = df.randomSplit([0.75, 0.25], seed=123)\n",
    "\n",
    "# Define the stages of the pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"review_body\", outputCol=\"review_words\")\n",
    "stopwordremoverbody = StopWordsRemover(inputCol='review_words', outputCol='filtered_review_words')\n",
    "titleTokenizer = Tokenizer(inputCol=\"review_title\", outputCol=\"title_words\")\n",
    "stopwordremovertitle = StopWordsRemover(inputCol='title_words', outputCol='filtered_title_words')\n",
    "indexer = StringIndexer(inputCol=\"product_category\", outputCol=\"label\").setHandleInvalid('keep')\n",
    "idIndexer = StringIndexer(inputCol=\"product_id\", outputCol=\"id_feature\").setHandleInvalid('keep')\n",
    "hashingTF = HashingTF(inputCol=\"filtered_review_words\", outputCol=\"review_features\", numFeatures=262144)\n",
    "titleHashingTF = HashingTF(inputCol=\"filtered_title_words\", outputCol=\"title_features\", numFeatures=262144)\n",
    "assembler = VectorAssembler(inputCols=[\"id_feature\", \"review_features\", \"title_features\"], outputCol=\"features\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwordremoverbody, titleTokenizer, stopwordremovertitle, indexer, idIndexer, hashingTF, titleHashingTF, assembler, lr])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model = pipeline.fit(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.349406\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing set\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "played around with few iterations. considered logistic regression model where i combine product id, review title, and review body in singular column called 'words'. considered an idf with stop words. decided against that as perhaps some of the stop words would be useful. \n",
    "\n",
    "the highest accuracy i got was combining review body, review title, product id into a words column, used stopwords remover and then attempted to get product_category. only got 40% accuracy however....y\n",
    "\n",
    "attempted a lean model with just the review body and product category and got 30% accuracy.\n",
    "\n",
    "probably would need to spend time tuning parameters. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `b.` predict customer ratings\n",
    "note: thought about using stop word however realised words like not etc may provide valuable data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, Tokenizer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "data = df.select(col(\"review_id\"), col(\"product_id\"), col(\"reviewer_id\"), col(\"stars\").cast(\"double\"), col(\"review_body\"))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3], seed=1234)\n",
    "\n",
    "# Convert string columns to index values, # Combine all features into a single feature vector\n",
    "tokenizer = Tokenizer(inputCol=\"review_body\", outputCol=\"review_words\")\n",
    "hashingTF = HashingTF(inputCol=\"review_words\", outputCol=\"review_features\", numFeatures=1000)\n",
    "productIdIndexer = StringIndexer(inputCol=\"product_id\", outputCol=\"product_id_index\").setHandleInvalid('keep')\n",
    "reviewerIdIndexer = StringIndexer(inputCol=\"reviewer_id\", outputCol=\"reviewer_id_index\").setHandleInvalid('keep')\n",
    "assembler = VectorAssembler(inputCols=[\"product_id_index\", \"reviewer_id_index\", \"review_features\"], outputCol=\"features\")\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"stars\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "#pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, productIdIndexer, reviewerIdIndexer, assembler, lr])\n",
    "\n",
    "\n",
    "# Fit the model \n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"stars\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(predictionCol='prediction', labelCol='stars', metricName='r2')\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "print('R-Squared (R2):', r2)\n",
    "print(\"Root Mean Squared Error (RMSE) = %g\" % rmse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Load the data into a Spark DataFrame\n",
    "training_reviews = spark.read.json(file_location)\n",
    "training_reviews = training_reviews.na.drop()\n",
    "test_reviews = spark.read.json('./data/train_test/dataset_en_test.json')\n",
    "test_reviews = test_reviews.na.drop()\n",
    "\n",
    "training_reviews = training_reviews.withColumn('stars', training_reviews['stars'].cast('float'))\n",
    "test_reviews = test_reviews.withColumn('stars', test_reviews['stars'].cast('float'))\n",
    "\n",
    "# Transform reviewer_id and product_id columns into indices\n",
    "reviewer_id_indexer = StringIndexer(inputCol=\"reviewer_id\", outputCol=\"reviewer_id_index\").setHandleInvalid('keep')\n",
    "product_id_indexer = StringIndexer(inputCol=\"product_id\", outputCol=\"product_id_index\").setHandleInvalid('keep')\n",
    "training_reviews = reviewer_id_indexer.fit(training_reviews).transform(training_reviews)\n",
    "training_reviews = product_id_indexer.fit(training_reviews).transform(training_reviews)\n",
    "test_reviews = reviewer_id_indexer.fit(test_reviews).transform(test_reviews)\n",
    "test_reviews = product_id_indexer.fit(test_reviews).transform(test_reviews)\n",
    "\n",
    "# Train a collaborative filtering model using ALS algorithm\n",
    "als = ALS(rank=10, maxIter=5, regParam=0.01, userCol=\"reviewer_id_index\", itemCol=\"product_id_index\", ratingCol=\"stars\")\n",
    "model = als.fit(training_reviews)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "predictions = model.transform(test_reviews)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='stars', predictionCol='prediction')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print('Root Mean Squared Error =', rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top 5 product recommendations for each customer\n",
    "userRecs = model.recommendForAllUsers(5)\n",
    "\n",
    "userRecs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify the most popular products based on the number of reviews and ratings\n",
    "most_popular_products = test_reviews.groupBy([\"product_id\", 'product_category']).agg({\"stars\": \"mean\", \"review_id\": \"count\"}).withColumnRenamed(\"avg(stars)\", \"avg_rating\").withColumnRenamed(\"count(review_id)\", \"num_reviews\").orderBy(col(\"num_reviews\").desc()).show(25)\n",
    "\n",
    "# 2. Explore the performance of the model on different subsets of the data\n",
    "perf_by_language = predictions.groupBy(\"language\").agg({\"stars\": \"mean\", \"prediction\": \"mean\", \"review_id\": \"count\"}).withColumnRenamed(\"avg(stars)\", \"avg_actual_rating\").withColumnRenamed(\"avg(prediction)\", \"avg_predicted_rating\").withColumnRenamed(\"count(review_id)\", \"num_reviews\").orderBy(col(\"num_reviews\").desc())\n",
    "perf_by_language.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
